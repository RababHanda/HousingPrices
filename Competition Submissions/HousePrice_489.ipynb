{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43200a7e",
   "metadata": {},
   "source": [
    "## MMA 860 Team Project: Predicting Housing Prices\n",
    "\n",
    "Team Istanbul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7e3e1aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install openpyxl\n",
    "# %matplotlib inline\n",
    "# %pip install statsmodels\n",
    "# %pip install scikit-learn seaborn\n",
    "# %pip install jupyter_contrib_nbextensions\n",
    "# %pip install --upgrade scikit-learn\n",
    "# %pip install lightgbm xgboost scikit-learn --quiet\n",
    "# %pip install missingno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4c3f8985",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-19T23:23:32.795114Z",
     "iopub.status.busy": "2025-04-19T23:23:32.794783Z",
     "iopub.status.idle": "2025-04-19T23:23:34.802884Z",
     "shell.execute_reply": "2025-04-19T23:23:34.801851Z"
    },
    "papermill": {
     "duration": 2.01369,
     "end_time": "2025-04-19T23:23:34.804955",
     "exception": false,
     "start_time": "2025-04-19T23:23:32.791265",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import missingno as msno\n",
    "\n",
    "import statsmodels.imputation.mice as mice\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "from patsy import dmatrices\n",
    "import scipy.stats as stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "44d1b4cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T23:23:34.810609Z",
     "iopub.status.busy": "2025-04-19T23:23:34.810210Z",
     "iopub.status.idle": "2025-04-19T23:25:19.776415Z",
     "shell.execute_reply": "2025-04-19T23:25:19.775162Z"
    },
    "papermill": {
     "duration": 104.970876,
     "end_time": "2025-04-19T23:25:19.778154",
     "exception": false,
     "start_time": "2025-04-19T23:23:34.807278",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Converting data source to dataframes\n",
    "file_path_test  = \"test.csv\"\n",
    "file_path_train = \"train.csv\"\n",
    "\n",
    "test  = pd.read_csv(file_path_test)\n",
    "train = pd.read_csv(file_path_train)\n",
    "\n",
    "data = pd.concat([train, test], sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b9eca4",
   "metadata": {},
   "source": [
    "### Data Cleaning and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e4c061e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill categorical with mode\n",
    "cat_cols = data.select_dtypes(include='object').columns\n",
    "for col in cat_cols:\n",
    "    data[col] = data[col].fillna(data[col].mode()[0])\n",
    "    lbl = LabelEncoder()\n",
    "    data[col] = lbl.fit_transform(data[col].astype(str))\n",
    "\n",
    "# Fill numerical with median\n",
    "num_cols = data.select_dtypes(include=['int64', 'float64']).columns\n",
    "for col in num_cols:\n",
    "    data[col] = data[col].fillna(data[col].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36eb941f",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1af8d858",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining features for Bathrooms to reduce dimensions\n",
    "data['TotalSF'] = data['TotalBsmtSF'] + data['1stFlrSF'] + data['2ndFlrSF']\n",
    "data['TotalBath'] = (data['FullBath'] + data['HalfBath'] * 0.5 +\n",
    "                     data['BsmtFullBath'] + data['BsmtHalfBath'] * 0.5)\n",
    "\n",
    "#Combining features for porches to reduce dimensions\n",
    "data['TotalPorchSF'] = (data['OpenPorchSF'] + data['EnclosedPorch'] +\n",
    "                        data['3SsnPorch'] + data['ScreenPorch'])\n",
    "\n",
    "#Converting numerical features to Categorical (binary features make presence/effect of features more explicit)\n",
    "data['HasPool'] = (data['PoolArea'] > 0).astype(int)\n",
    "data['HasGarage'] = (data['GarageArea'] > 0).astype(int)\n",
    "data['HasFireplace'] = (data['Fireplaces'] > 0).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf38b93",
   "metadata": {},
   "source": [
    "### Regression Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5f4d96a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into train & test sets\n",
    "train_clean = data[:len(train)].copy()\n",
    "test_clean = data[len(train):].copy()\n",
    "train_clean['SalePrice'] = train['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "983390bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Log transforming target to reduce skewness in data\n",
    "y = np.log1p(train_clean['SalePrice'])\n",
    "\n",
    "X = train_clean.drop(['Id', 'SalePrice'], axis=1)\n",
    "X_test = test_clean.drop(['Id', 'SalePrice'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5925f8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining base models\n",
    "Using pipeline method to chain steps & uses features in similar scaling\n",
    "ridge = make_pipeline(RobustScaler(), Ridge(alpha=15))\n",
    "lasso = make_pipeline(RobustScaler(), Lasso(alpha=0.0005))\n",
    "\n",
    "#Usring XBG & LGB models to handle complex feature and target relationships\n",
    "xgb = XGBRegressor(n_estimators=1000, learning_rate=0.05, max_depth=3,\n",
    "                   subsample=0.7, colsample_bytree=0.7, random_state=42)\n",
    "lgbm = LGBMRegressor(objective='regression', num_leaves=5, learning_rate=0.05,\n",
    "                     n_estimators=1000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "66302c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stacking all models\n",
    "stacked_model = StackingRegressor(\n",
    "    estimators=[('ridge', ridge), ('lasso', lasso), ('xgb', xgb), ('lgbm', lgbm)],\n",
    "    final_estimator=Ridge(alpha=10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397f8116",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "270dced0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating Root Mean Square Error \n",
    "def rmse_cv(model):\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    rmse = -cross_val_score(model, X, y, scoring=\"neg_root_mean_squared_error\", cv=kf)\n",
    "    return rmse.mean()\n",
    "\n",
    "print(f\"Stacked Model CV RMSE: {rmse_cv(stacked_model):.5f}/n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e189ef4",
   "metadata": {},
   "source": [
    "### Predict Values for Competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "abe26978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000594 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3889\n",
      "[LightGBM] [Info] Number of data points in the train set: 1460, number of used features: 78\n",
      "[LightGBM] [Info] Start training from score 12.024057\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000534 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3596\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 76\n",
      "[LightGBM] [Info] Start training from score 12.021409\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000397 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3611\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 77\n",
      "[LightGBM] [Info] Start training from score 12.023288\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000730 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3609\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 77\n",
      "[LightGBM] [Info] Start training from score 12.020737\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000402 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3626\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 76\n",
      "[LightGBM] [Info] Start training from score 12.032956\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000413 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3612\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 78\n",
      "[LightGBM] [Info] Start training from score 12.021897\n"
     ]
    }
   ],
   "source": [
    "#Fit and predict\n",
    "stacked_model.fit(X, y)\n",
    "final_preds = np.expm1(stacked_model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "17b73102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file saved: submission.csv\n"
     ]
    }
   ],
   "source": [
    "#Export csv with predictions for comptetion submission\n",
    "submission = pd.DataFrame({\n",
    "    'Id': test['Id'],\n",
    "    'SalePrice': final_preds\n",
    "})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"Submission file saved: submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 868283,
     "sourceId": 5407,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 113.073528,
   "end_time": "2025-04-19T23:25:20.906900",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-19T23:23:27.833372",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
